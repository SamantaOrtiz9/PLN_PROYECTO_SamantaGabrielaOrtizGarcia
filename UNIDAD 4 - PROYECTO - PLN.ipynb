{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2132627",
   "metadata": {},
   "source": [
    "# UNIDAD 4 - PROYECTO: ANÁLISIS DE SENTIMIENTOS Y PREDICCIÓN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4a441",
   "metadata": {},
   "source": [
    "## ALUMNO: Samanta Gabriela Ortiz Garcia\n",
    "## NÚMERO DE CONTROL: 20120140\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9336d5c6",
   "metadata": {},
   "source": [
    "## PRESENTACIÓN.\n",
    "\n",
    "El siguiente proyecto consiste en emplear el análisis de sentimientos para tratar de predecir las calificaciones de una serie.\n",
    "\n",
    "El dataset es sobre las calificaciones que recibió la adaptación al \"live-action\" el anime \"OnePiece\".\n",
    "\n",
    "El dataset contiene las reseñas que se recibieron en la plataforma de Netflix, el dataset contiene información como:\n",
    "\n",
    "* Título.\n",
    "* Reseña.\n",
    "* Fecha.\n",
    "* Calificación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d06a4",
   "metadata": {},
   "source": [
    "## ETAPA 1: ANÁLISIS EXPLORATORIO DE LOS DATOS.\n",
    "\n",
    "### Paso 1. Importe las librerías necesarias (pandas, numpy, seaborn, nltk, etc...)\n",
    "\n",
    "### Paso 2. Cargue y muestre información del dataset; muestre información estadística de las columnas numéricas.\n",
    "\n",
    "### Paso 3. Identifique los datos nulos: muestre las filas que contienen datos nulos (no se deben tratar aún).\n",
    "\n",
    "### Paso 4. Muestre la distribución de la columna \"Rating\", haga un análisis de la distribución.\n",
    "\n",
    "### Paso 5. Identifique si alguna de las columnas se puede convertir en categórica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e511f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASO 1\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as amano\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45cfb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n",
      "                                       Title  \\\n",
      "0  Never seen the anime and still enjoyed it   \n",
      "1     Not My Kind of Show. But This I Loved.   \n",
      "2         I'm Shocked By How Good This Is...   \n",
      "3             See Netflix?? You CAN do it...   \n",
      "4                           WE WANT SEASON 2   \n",
      "\n",
      "                                              Review              Date  Rating  \n",
      "0  I have to admit that I've never watched any of...  4 September 2023     8.0  \n",
      "1  I'm a 60yo man & old school like the The Marin...  4 September 2023     9.0  \n",
      "2  Every live action anime that I've briefly seen...  1 September 2023     9.0  \n",
      "3  This is bar none one of the best live-action a...  4 September 2023    10.0  \n",
      "4  Being a one piece fan myself, I was a bit inse...    31 August 2023    10.0  \n",
      "\n",
      "Información del dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 878 entries, 0 to 877\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Title   878 non-null    object \n",
      " 1   Review  878 non-null    object \n",
      " 2   Date    878 non-null    object \n",
      " 3   Rating  870 non-null    float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 27.6+ KB\n",
      "None\n",
      "\n",
      "Estadísticas de las columnas numéricas:\n",
      "           Rating\n",
      "count  870.000000\n",
      "mean     8.517241\n",
      "std      2.268826\n",
      "min      1.000000\n",
      "25%      8.000000\n",
      "50%      9.000000\n",
      "75%     10.000000\n",
      "max     10.000000\n"
     ]
    }
   ],
   "source": [
    "#PASO 2\n",
    "dataset = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(dataset.head())\n",
    "\n",
    "print(\"\\nInformación del dataset:\")\n",
    "print(dataset.info())\n",
    "\n",
    "print(\"\\nEstadísticas de las columnas numéricas:\")\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a22a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas con datos nulos:\n",
      "                                                 Title  \\\n",
      "143    I didn't expect much, but I absolutely love it.   \n",
      "145                   Thank you for heading this, Oda!   \n",
      "447  DIFFERENT MEDIA MEANS DIFFERENT STORYTELLING A...   \n",
      "500                     I'm not sure about rating yet!   \n",
      "520                                  Great adaptation!   \n",
      "540                  My disappointment is immeasurable   \n",
      "569                                       A must watch   \n",
      "782                          Emily Rudd's hair as Nami   \n",
      "\n",
      "                                                Review               Date  \\\n",
      "143  I admit, when I write reviews it's usually whe...  25 September 2023   \n",
      "145  Fun fact: Eiichiro Oda, the creator of the man...  12 September 2023   \n",
      "447  Just because there are many scenes in the mang...   1 September 2023   \n",
      "500  I like the characters so far except Sanji's ha...   4 September 2023   \n",
      "520  One Piece somehow managed to pull it off: a le...   8 September 2023   \n",
      "540  As a life long fan of One Piece I don't know w...     31 August 2023   \n",
      "569  If you browse there are series with more actio...   2 September 2023   \n",
      "782  Everything went so right with casting and thei...   1 September 2023   \n",
      "\n",
      "     Rating  \n",
      "143     NaN  \n",
      "145     NaN  \n",
      "447     NaN  \n",
      "500     NaN  \n",
      "520     NaN  \n",
      "540     NaN  \n",
      "569     NaN  \n",
      "782     NaN  \n"
     ]
    }
   ],
   "source": [
    "#PASO 3\n",
    "null_rows = dataset[dataset.isnull().any(axis=1)]\n",
    "\n",
    "print(\"Filas con datos nulos:\")\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32a764a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAGPCAYAAACqMyKIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfK0lEQVR4nO3de7RkZX3m8e9DcxWQi90icvVCVEwQTIsYzcQEM6BBMDoKZqJgROIER2fFJIIxai4kjJM4MTGuiIq2EsWGGEEnFy4RE6NyRxGQ2IpAy60BuWpA2t/8sXeb4ni6u07T+9Spt7+ftXpV1burdj11+nQ/td+9a1eqCkmSNN02m3QASZL0yFnokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0aSNI8tdJfm8jrWvPJPclWdTfviDJsRtj3TOe574kT5wxtlmSs5L82kZ8no8k+aONtb5HIsk/JDl60jmkIWw+6QDSQpfk28AuwEPAauBq4KPAKVX1Q4Cqev0c1nVsVZ23tvtU1Q3Ado8s9fpV1WzPcRJwflWdOvTzjyNJAd8DCrgb+CTw21W1eozHvhN4clX96pqxqnrhQFGlibPQpfG8uKrOS7ID8HPAe4BnA6/ZmE+SZPOqemhjrnMuqurEST33OjyjqlYkeTLweeAa4AMTziQtOE65S3NQVXdX1dnAkcDRSX4SHj6tnGRxks8muSvJnUn+tZ/K/hiwJ/CZfrr7d5LsnaSSvDbJDcA/j4yNvuF+UpKLktzdT4nv3D/X85OsHM2Y5NtJXtBfX5TkrUm+meTeJJcm2aNfVn1JkmSHJB9NsirJ9UnelmSzftkxSb6Q5E+TfDfJdUnWuqWb5IAkl/XP90lg6xnLD0tyRf/z+WKS/cb82a8A/g3Yf2Rd70lyY5J7+tf2s/34ocBbgSP7n/VX+vEf7b5Y3+tK8oQk/9K/jvOS/FWS0/plWyc5Lckd/eu4OMku47wOaSgWurQBquoiYCXws7MsfnO/bAndVP1bu4fUq4Ab6Lb2t6uqd4085ueApwGHrOUpXw38GvB4uqn/vxgz6m8CrwReBDy6X8f3ZrnfXwI7AE/ss7yah88+PBu4FlgMvAv4UJLMXEmSLYFPAx8DdgbOAF42svyZwKnArwOPAd4PnJ1kq/W9kCRPpft5rxgZvpiu4HcGPg6ckWTrqvpH4I+BT/Y/62esZbXrel0fBy7qc74TeNXI446m+3nt0S9/PfD99b0GaUgWurThbqIrkpl+AOwK7FVVP6iqf631f2nCO6vq/qpaWyl8rKq+VlX3A78HvGLNQXPrcSzwtqq6tjpfqao7Ru/Qr+dI4MSqureqvg38GQ8vsOur6gP9vutl/eubbYv0IGAL4M/7134mXemu8Trg/VV1YVWtrqplwAP949bmsiT30021XwC8b82Cqjqtqu6oqoeq6s+ArYCnrPensp7XlWRP4FnA26vqwar6AnD2yON+QFfkT+5fx6VVdc8cnlfa6Cx0acPtBtw5y/j/oduKPCfJt5KcMMa6bpzD8uvpSnPxGOvdA/jmeu6zGNiyX+/oc+w2cvuWNVeqas0W/mwH1T0e+M6MNzCj690LeHM/TX1Xkrv6jI9fR75n9s91JN0W9bZrFiR5c5Jr+l0Rd9FtNY/zc1ljba/r8cCdI2Pw8L+DjwH/BJye5KYk70qyxRyeV9roLHRpAyR5Fl3hfWHmsn4r981V9UTgxcBvJjl4zeK1rHJ9W/B7jFzfk24L8XbgfuBRI7kW0U31r3Ej8KT1rPv2fn17zXiO76zncbO5GdhtxnT8njPynFRVO478eVRVfWJdK+1nF5YDXwLeDtDvL38L8Apgp6rake5I+DXP/Ui+SvJmYOckjxoZ+9HfQT/78PtVtS/wM8BhdLsppImx0KU5SPLoJIcBpwOnVdWVs9znsCRP7kvtHrqPuq35mNWtdPup5+pXk+zbF8wfAGf208T/Dmyd5Jf6LcS30U07r/FB4A+T7JPOfkkeM7rifj3LgZOSbJ9kL7p976dtQM4v0e3jf2OSzZO8FDhwZPkHgNcneXafZ9s++/Zjrv9k4LgkjwO2759rFbB5krfTHSewxq3A3msO7puLqroeuAR4Z5ItkzyH7s0ZAEl+PslP9W+g7qF7Q7Tej9JJQ7LQpfF8Jsm9dFuYvwu8m7V/ZG0f4DzgPrqCe19VXdAv+xPgbf1082/N4fk/BnyEbop4a+CN0B11D/wGXXF/h26LffSo93fTlfU5dMXzIWCbWdb/P/vHfotu1uHjdAevzUlVPQi8FDgG+C7dNPmnRpZfQrcf/b398hX9fcdd/5V0H137bbop73+ge1NzPfAfPHxa/Iz+8o4kl831tQD/HXgOcAfwR3SfgX+gX/Y44Ey6n+k1faYNeQMkbTRZ/7E6kqT+I3hfr6p3TDqLNBu30CVpFkmeleRJ6c4hcChwBN1H8qQFyTPFSdLsHke3u+AxdLsx/kdVXT7ZSNLaOeUuSVIDnHKXJKkBUz3lvnjx4tp7770nHUOSpHlz6aWX3l5VS2aOT3Wh77333lxyySWTjiFJ0rxJcv1s4065S5LUAAtdkqQGWOiSJDXAQpckqQEWuiRJDbDQJUlqgIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJasBUf32qJEnz5eTLbx9s3SccsPgRr8MtdEmSGmChS5LUAAtdkqQGWOiSJDXAQpckqQEWuiRJDbDQJUlqgIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0SZIaYKFLktQAC12SpAYMXuhJFiW5PMln+9s7Jzk3yTf6y51G7ntikhVJrk1yyNDZJElqxXxsob8JuGbk9gnA+VW1D3B+f5sk+wJHAU8HDgXel2TRPOSTJGnqDVroSXYHfgn44MjwEcCy/voy4CUj46dX1QNVdR2wAjhwyHySJLVi6C30Pwd+B/jhyNguVXUzQH/52H58N+DGkfut7MceJslxSS5JcsmqVasGCS1J0rQZrNCTHAbcVlWXjvuQWcbqxwaqTqmqpVW1dMmSJY8ooyRJrdh8wHU/Fzg8yYuArYFHJzkNuDXJrlV1c5Jdgdv6+68E9hh5/O7ATQPmkySpGYNtoVfViVW1e1XtTXew2z9X1a8CZwNH93c7Gjirv342cFSSrZI8AdgHuGiofJIktWTILfS1ORlYnuS1wA3AywGq6qoky4GrgYeA46tq9QTySZI0deal0KvqAuCC/vodwMFrud9JwEnzkUmSpJZ4pjhJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0SZIaYKFLktQAC12SpAZY6JIkNcBClySpARa6JEkNsNAlSWqAhS5JUgMsdEmSGmChS5LUAAtdkqQGWOiSJDXAQpckqQEWuiRJDbDQJUlqgIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0SZIaYKFLktQAC12SpAZY6JIkNcBClySpARa6JEkNsNAlSWqAhS5JUgMsdEmSGmChS5LUAAtdkqQGWOiSJDXAQpckqQEWuiRJDbDQJUlqgIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0SZIaYKFLktQAC12SpAZY6JIkNcBClySpARa6JEkNsNAlSWrAYIWeZOskFyX5SpKrkvx+P75zknOTfKO/3GnkMScmWZHk2iSHDJVNkqTWDLmF/gDwC1X1DGB/4NAkBwEnAOdX1T7A+f1tkuwLHAU8HTgUeF+SRQPmkySpGYMVenXu629u0f8p4AhgWT++DHhJf/0I4PSqeqCqrgNWAAcOlU+SpJYMug89yaIkVwC3AedW1YXALlV1M0B/+dj+7rsBN448fGU/NnOdxyW5JMklq1atGjK+JElTY9BCr6rVVbU/sDtwYJKfXMfdM9sqZlnnKVW1tKqWLlmyZCMllSRpus3LUe5VdRdwAd2+8VuT7ArQX97W320lsMfIw3YHbpqPfJIkTbshj3JfkmTH/vo2wAuArwNnA0f3dzsaOKu/fjZwVJKtkjwB2Ae4aKh8kiS1ZPMB170rsKw/Un0zYHlVfTbJl4DlSV4L3AC8HKCqrkqyHLgaeAg4vqpWD5hPkqRmDFboVfVV4IBZxu8ADl7LY04CThoqkyRJrfJMcZIkNcBClySpARa6JEkNsNAlSWqAhS5JUgMsdEmSGmChS5LUAAtdkqQGWOiSJDXAQpckqQEWuiRJDbDQJUlqgIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0SZIaYKFLktSAsQo9yUFJLk5yX5IHk6xOcs/Q4SRJ0njG3UJ/L/BK4BvANsCxwF8OFUqSJM3N5uPesapWJFlUVauBDyf54oC5JEnSHIxb6N9LsiVwRZJ3ATcD2w4XS5IkzcW4U+6vAhYBbwDuB/YAXjZUKEmSNDdjbaFX1fX91e8Dvz9cHEmStCHWWehJllfVK5JcCdTM5VW132DJJEnS2Na3hf6m/vKwoYNIkqQNt85Cr6qb+6ubATdX1X8AJNkG2GXgbJIkaUzjHhR3BvDDkdur+zFJkrQAjFvom1fVg2tu9Ne3HCaSJEmaq3ELfVWSw9fcSHIEcPswkSRJ0lyNe2KZ1wN/k+S9QIAbgVcPlkqSJM3JuJ9D/yZwUJLtgFTVvcPGkiRJczFWoSfZiu7McHsDmycBoKr+YLBkkiRpbONOuZ8F3A1cCjwwXBxJkrQhxi303avq0EGTSJKkDTbuUe5fTPJTgyaRJEkbbNwt9OcBxyS5jm7KPUB5LndJkhaGcQv9hYOmkCRJj8hYU+7916fuAfxCf/174z5WkiQNb6xSTvIO4C3Aif3QFsBpQ4WSJElzM+5W9i8DhwP3A1TVTcD2Q4WSJElzM26hP1hVBRRAkm2HiyRJkuZq3EJfnuT9wI5JXgecB3xguFiSJGkuxj2X+58m+UXgHuApwNur6txBk0mSpLGN+7E1+gK3xCVJWoDG/XKWe+n3nwNb0h3lfn9VPXqoYJIkaXzjTrk/7Ij2JC8BDhwikCRJmrsNOjlMVX0a+IWNG0WSJG2ocafcXzpyczNgKf85BS9JkiZs3IPiXjxy/SHg28ARGz2NJEnaIOPuQ3/N0EEkSdKGG/dc7suS7Dhye6ckpw6WSpIkzcm4B8XtV1V3rblRVd8FDhgkkSRJmrNxC32zJDutuZFkZ+ZwUhpJkjSscUv5z4AvJjmT7uj2VwAnDZZKkiTNybgHxX00ySV0nz0P8NKqunrQZJIkaWxzObHMznSne/1LYFWSJwyUSZIkzdG4R7m/A3gLcGI/tAVw2lChJEnS3Iy7hf7LwOHA/QBVdROw/boekGSPJJ9Lck2Sq5K8qR/fOcm5Sb7RX44ebHdikhVJrk1yyIa9JEmSNj3jFvqDVVX0p3tNsu0Yj3kIeHNVPQ04CDg+yb7ACcD5VbUPcH5/m37ZUcDTgUOB9yVZNJcXI0nSpmrcQl+e5P3AjkleB5wHfGBdD6iqm6vqsv76vcA1wG50p4xd1t9tGfCS/voRwOlV9UBVXQeswG90kyRpLOs9yj1JgE8CTwXuAZ4CvL2qzh33SZLsTXcimguBXarqZuhKP8lj+7vtBnx55GEr+7GZ6zoOOA5gzz33HDeCJElNW2+hV1Ul+XRV/TQwdomvkWQ74G+B/1VV93TvD2a/62xPP0ueU4BTAJYuXeo3vkmSxPhT7l9O8qy5rjzJFnRl/jdV9al++NYku/bLdwVu68dXAnuMPHx34Ka5PqckSZuicQv95+lK/ZtJvprkyiRfXdcD+qn6DwHXVNW7RxadDRzdXz8aOGtk/KgkW/Wfcd8HuGjcFyJJ0qZsnVPuSfasqhuAF27Aup8LvAq4MskV/dhbgZPpDrJ7LXAD8HKAqroqyXLgaroj5I+vqtUb8LySJG1y1rcP/dPAM6vq+iR/W1UvG3fFVfUFZt8vDnDwWh5zEp4jXpKkOVvflPtoIT9xyCCSJGnDra/Qay3XJUnSArK+KfdnJLmHbkt9m/46/e2qqkcPmk6SJI1lnYVeVZ56VZKkKTCXr0+VJEkLlIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSA9Z3LndJkgZx8uW3D7LeEw5YPMh6Fzq30CVJaoCFLklSAyx0SZIaYKFLktQAC12SpAZY6JIkNcBClySpARa6JEkNsNAlSWqAhS5JUgMsdEmSGmChS5LUAAtdkqQGWOiSJDXAQpckqQEWuiRJDbDQJUlqgIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0SZIaYKFLktQAC12SpAZY6JIkNcBClySpARa6JEkNsNAlSWqAhS5JUgMsdEmSGmChS5LUAAtdkqQGWOiSJDXAQpckqQEWuiRJDbDQJUlqgIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBgxV6klOT3JbkayNjOyc5N8k3+sudRpadmGRFkmuTHDJULkmSWjTkFvpHgENnjJ0AnF9V+wDn97dJsi9wFPD0/jHvS7JowGySJDVlsEKvqn8B7pwxfASwrL++DHjJyPjpVfVAVV0HrAAOHCqbJEmtme996LtU1c0A/eVj+/HdgBtH7reyH5MkSWNYKAfFZZaxmvWOyXFJLklyyapVqwaOJUnSdJjvQr81ya4A/eVt/fhKYI+R++0O3DTbCqrqlKpaWlVLlyxZMmhYSZKmxXwX+tnA0f31o4GzRsaPSrJVkicA+wAXzXM2SZKm1uZDrTjJJ4DnA4uTrATeAZwMLE/yWuAG4OUAVXVVkuXA1cBDwPFVtXqobJIktWawQq+qV65l0cFruf9JwElD5ZEkqWWDFbokaf6cfPntg6z3hAMWD7JebXwL5Sh3SZL0CFjokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0SZIaYKFLktQAC12SpAZY6JIkNcBClySpARa6JEkNsNAlSWqAhS5JUgMsdEmSGmChS5LUAAtdkqQGWOiSJDXAQpckqQEWuiRJDbDQJUlqgIUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ3YfNIBJGmhOfny2wdb9wkHLB5s3dq0uYUuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoCFLklSAyx0SZIaYKFLktQAC12SpAZY6JIkNcBClySpARa6JEkNsNAlSWqAhS5JUgM2n3SA+XLy5bcPtu4TDlg82LqlmYb6Xfb3WJpum0yhS5oc34RIw3PKXZKkBljokiQ1wEKXJKkBFrokSQ2w0CVJaoBHuWuj8mhmSZoMt9AlSWqAhS5JUgOccl/AnL6WJI3LQtcmzVMCS2qFU+6SJDXAQpckqQEWuiRJDbDQJUlqwIIr9CSHJrk2yYokJ0w6jyRJ02BBFXqSRcBfAS8E9gVemWTfyaaSJGnhW1CFDhwIrKiqb1XVg8DpwBETziRJ0oKXqpp0hh9J8t+AQ6vq2P72q4BnV9UbRu5zHHBcf/MpwLUDxVkMDPch5Y1v2vKCmefDtOWF6cs8bXnBzPNhyLx7VdWSmYML7cQymWXsYe84quoU4JTBgySXVNXSoZ9nY5m2vGDm+TBteWH6Mk9bXjDzfJhE3oU25b4S2GPk9u7ATRPKIknS1FhohX4xsE+SJyTZEjgKOHvCmSRJWvAW1JR7VT2U5A3APwGLgFOr6qoJxRl8Wn8jm7a8YOb5MG15YfoyT1teMPN8mPe8C+qgOEmStGEW2pS7JEnaABa6JEkNsNAlSWqAhS5JUgMW1FHumpskuwC70Z1856aqunXCkdZr2jJPW14w83yYtrwwfZmnLe9C4FHuI6blFyjJ/sBfAzsA3+mHdwfuAn6jqi6bTLK1m7bM05YXzDwfpi0vTF/macu7oFTVJv8H2B/4MnANcF7/5+v92DMnnW+WvFfQneN+5vhBwFcmna+FzNOW18zmbSXztOWdkfGpwFuAvwDe019/2nw9v1PunY8Av15VF44OJjkI+DDwjEmEWodtZ2YFqKovJ9l2EoHGMG2Zpy0vmHk+TFtemL7M05YXgCRvAV5J9y2hF/XDuwOfSHJ6VZ08dAYLvTNtv0D/kOT/AR8FbuzH9gBeDfzjxFKt27Rlnra8YOb5MG15YfoyT1veNV4LPL2qfjA6mOTdwFXA4IXuPnQgyV8AT2L2X6DrauTrWxeKJC+k+6743ei+pW4lcHZV/f1Eg63DtGWetrxg5vkwbXlh+jJPW16AJF8HDqmq62eM7wWcU1VPGTyDhd6Zxl8gSdLCkORQ4L3AN/jPDcM9gScDb6iqwWcXLPTGJDmuuu+MnxrTlnna8oKZ58O05YXpy7zQ8ybZDDiQh28YXlxVq+fj+d2Hvh4L/RdoFpl0gA0wbZmnLS+YeT5MW16YvswLOm9V/ZDu01ET4Rb6eiT59ap6/6RzzJTkqXTvAi+sqvtGxg+dj6mdDZHkQKCq6uIk+wKHAl+flt0aST5aVa+edI5xJXke3dbC16rqnEnnmU2SZwPXVNU9SbYBTgCeCVwN/HFV3T3RgDMkeSPwd1V143rvvEAk2RI4iu7cGucl+RXgZ+g+pnvKzIO4FoIkTwJ+me5YpofoprE/sdB+H8aV5LNVddjgz2Ohr1uS11TVhyedY1T/n8rxdP8g9wfeVFVn9csuq6pnTjDerJK8A3gh3azQucCzgQuAFwD/VFUnTS7dj0ty9swh4OeBfwaoqsPnPdR6JLmoqg7sr7+O7nfk74D/CnxmPj42M1dJrgKeUVUPJTkF+B5wJnBwP/7SiQacIcndwP3AN4FPAGdU1arJplq3JH9D9+/uUXQnZ9kO+BTdz5iqOmZS2WbT///2YuDzwIvoPpf+XbqC/42qumBi4TZQkl2r6ubBn8dCX7ckN1TVnpPOMSrJlcBzquq+JHvT/Qf4sap6T5LLq+qAySb8cX3m/YGtgFuA3Ue2yi6sqv0mmW+mJJfRbSV+kO7MgaH7D/wogKr6/OTSzW707z7JxcCLqmpV/9HLL1fVT0024Y9Lck1VPa2//rA3o0muqKr9JxZuFkkuB36a7o3okcDhwKV0vxufqqp7JxhvVkm+WlX7Jdmc7sxrj6+q1UlCd6KWhfZv70pg/z7jo4C/r6rnJ9kTOGsh/v+2ULgPne4Xfm2LgF3mM8uYFq2ZZq+qbyd5PnBm//GIhbqP6aH+wJDvJflmVd0DUFXfT/LDCWebzVLgTcDvAr9dVVck+f5CLPIRmyXZie5Ll7Jmy7Gq7k/y0GSjrdXXRmbBvpJkaVVdkuQngAU3FUy3y+iHwDnAOUm2oJt5eiXwp8CSSYZbi836afdt6bbSdwDupHtzvcUkg63D5sBquozbA1TVDf3Pe0FKsgNwIvAS/vP34DbgLODkqrpr6AwWemcX4BC6aZ1RAb44/3HW65Yk+1fVFQD9lvphwKnAgtsK6z2Y5FFV9T26LRzgR/8IFlyh9/9p/98kZ/SXt7Lw/73sQLe1GKCSPK6qbkmyHQv3jd6xwHuSvA24HfhSkhvpPvZz7ESTze5hP8d+//PZwNn9bNNC9CG6U1kvonuDekaSb9GdSvX0SQZbiw8CFyf5MvBfgP8NkGQJ3RuRhWo53S6551fVLQBJHgccDZwB/OLQAZxyB5J8CPhwVX1hlmUfr6pfmUCstUqyO90W7y2zLHtuVf3bBGKtU5KtquqBWcYXA7tW1ZUTiDW2JL8EPLeq3jrpLHPVT1vuUlXXTTrL2iTZHngi3ZumlbVwvxjpJ6rq3yedY66SPB6gqm5KsiPdLoMbquqidT5wQpI8HXga3QGdX590nnEkuXZtJ49Z17KNmsFClyTpkUlyDt0Xey1b84Y03Td4HgP8YlW9YOgMmw39BJIkbQKOBB4DfD7JnUnupPskz87Ay+cjgFvokiQNaL4+/myhS5I0oPn6+PNCP2pXkqQFbyF8/NlClyTpkZv4x58tdEmSHrnPAtutOT/IqCQXzEcA96FLktQAP7YmSVIDLHRJkhpgoUubqCSrk1yR5GtJPtOfEnRd998/yYtGbh+e5ITBg0oai/vQpU1Ukvuqarv++jLg39f1vfRJjgGWVtUb5imipDnwKHdJAF8C9gNIciDw58A2wPeB1wDXAX8AbJPkecCf9MuXVtUbknwEuIfua2cfB/xOVZ2ZZDPgvcDP9evYDDi1qs6cv5cmbRqccpc2cUkWAQfTfQ0odF+1+V+q6gDg7cAfV9WD/fVPVtX+VfXJWVa1K/A84DDg5H7spcDedF/reyzwnKFeh7Spcwtd2nRtk+QKusK9FDi3H98BWJZkH6CALcZc36f775G/uv+WKegK/ox+/JYkn9tY4SU9nFvo0qbr+1W1P7AXsCVwfD/+h8DnquongRcDW4+5vtHvu8+MS0kDs9ClTVxV3Q28EfitJFvQbaF/p198zMhd7wW2n+PqvwC8LMlm/Vb78x9ZWklrY6FLoqouB74CHAW8C/iTJP8GLBq52+eAffuPuh055qr/FlgJfA14P3AhcPdGCy7pR/zYmqRBJdmuqu5L8hjgIuC5VXXLpHNJrfGgOElD+2x/0potgT+0zKVhuIUuSVID3IcuSVIDLHRJkhpgoUuS1AALXZKkBljokiQ14P8DYJjFV1WIVYoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PASO 4\n",
    "plt.figure(figsize=(8, 6))\n",
    "dataset['Rating'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribución de Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617e77b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas que pueden convertirse en categóricas:\n",
      "Title     object\n",
      "Review    object\n",
      "Date      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#PASO 5\n",
    "column_types = dataset.dtypes\n",
    "\n",
    "categorical_columns = column_types[column_types == 'object']\n",
    "\n",
    "print(\"Columnas que pueden convertirse en categóricas:\")\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b59718",
   "metadata": {},
   "source": [
    "## ETAPA 2: ANÁLISIS DE SENTIMIENTOS.\n",
    "\n",
    "### Paso 1. Muestre las primeras 10 filas del dataset con las columnas \"Rating\" y \"Review\", haga un análisis rápido de esa información.\n",
    "\n",
    "### Paso 2. Haga una función que se encargue del pre-procesamiento:\n",
    "#### - Genere los tokens.\n",
    "#### - Filtre las palabras de parada.\n",
    "#### - Obtenga el lema de las palabras y guárdelo en una lista.\n",
    "#### - Retorne la lista en forma de una cadena, para ello debe unir los elementos de la lista mediante un espacio.\n",
    "\n",
    "### Paso 3. Aplique la función creada para obtener el lema de las columnas \"Review\" y \"Title\", guárde el resultado en nuevas columnas dentro del dataframe original (por ejemplo: \"ReviewText\", \"TitleText\").\n",
    "\n",
    "### Paso 4. Haga una función para obtener el sentimiento de las palabras, para ello puede utilizar el SentimentIntensityAnalizer() y su función \"polarity_scores()\". Al final debe retornar el puntaje de sentimiento.\n",
    "\n",
    "### Paso 5. Aplique la función creada para obtener el sentimiento en las columnas creadas en el paso 3, guarde el resultado en un par de columnas nuevas (por ejemplo: \"ReviewSentiment\", \"TitleSentiment\").\n",
    "\n",
    "### Paso 6. Prepare un dataframe con las columnas originales + las columnas creadas previamente, tendrían que haber 8 columnas, 3 de ellas deben ser numéricas (incluyendo \"Rating\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b360feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating                                             Review\n",
      "0     8.0  I have to admit that I've never watched any of...\n",
      "1     9.0  I'm a 60yo man & old school like the The Marin...\n",
      "2     9.0  Every live action anime that I've briefly seen...\n",
      "3    10.0  This is bar none one of the best live-action a...\n",
      "4    10.0  Being a one piece fan myself, I was a bit inse...\n",
      "5    10.0  GREAT ADAPTATION! As an old-school fan (since ...\n",
      "6     9.0  My kids play and watch One Piece anime and lov...\n",
      "7    10.0  I was so surprised that Netflix made something...\n",
      "8     8.0  Parenthetically I don't know anything about th...\n",
      "9    10.0  Initially I was apprehensive because of anime ...\n"
     ]
    }
   ],
   "source": [
    "#PASO 1\n",
    "subset = dataset[['Rating', 'Review']].head(10)\n",
    "\n",
    "print(subset)\n",
    "\n",
    "#El rating tiene variacion, algunas son calificaciones altas y otras bajas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb290608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review original: Este es un ejemplo review de palabras de parada y palabras de variacion.\n",
      "Review procesado: ejemplo review palabras parada palabras variacion\n"
     ]
    }
   ],
   "source": [
    "#PASO 2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def preprocess_review(review):\n",
    "   \n",
    "    tokens = word_tokenize(review.lower())\n",
    "\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "review_example = \"Este es un ejemplo review de palabras de parada y palabras de variacion.\"\n",
    "processed_review = preprocess_review(review_example)\n",
    "print(\"Review original:\", review_example)\n",
    "print(\"Review procesado:\", processed_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dab2f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          ReviewText  \\\n",
      "0  admit never watched animated show one piece ev...   \n",
      "1  man old school like marine admiral never heard...   \n",
      "2  every live action anime briefly seen nothing s...   \n",
      "3  bar none one best adaption animated source mat...   \n",
      "4  one piece fan bit insecured starting series bo...   \n",
      "\n",
      "                        TitleText  \n",
      "0  never seen anime still enjoyed  \n",
      "1                 kind show loved  \n",
      "2                    shocked good  \n",
      "3                     see netflix  \n",
      "4                     want season  \n"
     ]
    }
   ],
   "source": [
    "#PASO 3\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "dataset['ReviewText'] = dataset['Review'].apply(preprocess_text)\n",
    "dataset['TitleText'] = dataset['Title'].apply(preprocess_text)\n",
    "\n",
    "print(dataset[['ReviewText', 'TitleText']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98505b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Score: 0.5574\n"
     ]
    }
   ],
   "source": [
    "#PASO 4\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "def get_sentiment_score(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "review_example = \"This is a positive example.\"\n",
    "sentiment_score = get_sentiment_score(review_example)\n",
    "print(\"Sentiment Score:\", sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2664845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          ReviewText  ReviewSentiment  \\\n",
      "0  admit never watched animated show one piece ev...           0.8996   \n",
      "1  man old school like marine admiral never heard...           0.9887   \n",
      "2  every live action anime briefly seen nothing s...           0.9825   \n",
      "3  bar none one best adaption animated source mat...           0.8300   \n",
      "4  one piece fan bit insecured starting series bo...           0.9623   \n",
      "\n",
      "                        TitleText  TitleSentiment  \n",
      "0  never seen anime still enjoyed          0.5106  \n",
      "1                 kind show loved          0.8074  \n",
      "2                    shocked good          0.1531  \n",
      "3                     see netflix          0.0000  \n",
      "4                     want season          0.0772  \n"
     ]
    }
   ],
   "source": [
    "#PASO 5\n",
    "def get_sentiment_score(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "dataset['ReviewSentiment'] = dataset['ReviewText'].apply(get_sentiment_score)\n",
    "dataset['TitleSentiment'] = dataset['TitleText'].apply(get_sentiment_score)\n",
    "\n",
    "print(dataset[['ReviewText', 'ReviewSentiment', 'TitleText', 'TitleSentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c676cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Title  \\\n",
      "0  Never seen the anime and still enjoyed it   \n",
      "1     Not My Kind of Show. But This I Loved.   \n",
      "2         I'm Shocked By How Good This Is...   \n",
      "3             See Netflix?? You CAN do it...   \n",
      "4                           WE WANT SEASON 2   \n",
      "\n",
      "                                              Review              Date  \\\n",
      "0  I have to admit that I've never watched any of...  4 September 2023   \n",
      "1  I'm a 60yo man & old school like the The Marin...  4 September 2023   \n",
      "2  Every live action anime that I've briefly seen...  1 September 2023   \n",
      "3  This is bar none one of the best live-action a...  4 September 2023   \n",
      "4  Being a one piece fan myself, I was a bit inse...    31 August 2023   \n",
      "\n",
      "   Rating                                         ReviewText  ReviewSentiment  \\\n",
      "0     8.0  admit never watched animated show one piece ev...           0.8996   \n",
      "1     9.0  man old school like marine admiral never heard...           0.9887   \n",
      "2     9.0  every live action anime briefly seen nothing s...           0.9825   \n",
      "3    10.0  bar none one best adaption animated source mat...           0.8300   \n",
      "4    10.0  one piece fan bit insecured starting series bo...           0.9623   \n",
      "\n",
      "                        TitleText  TitleSentiment  \n",
      "0  never seen anime still enjoyed          0.5106  \n",
      "1                 kind show loved          0.8074  \n",
      "2                    shocked good          0.1531  \n",
      "3                     see netflix          0.0000  \n",
      "4                     want season          0.0772  \n"
     ]
    }
   ],
   "source": [
    "#PASO 6\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "dataset['ReviewText'] = dataset['Review'].apply(preprocess_text)\n",
    "dataset['TitleText'] = dataset['Title'].apply(preprocess_text)\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "dataset['ReviewSentiment'] = dataset['ReviewText'].apply(get_sentiment_score)\n",
    "dataset['TitleSentiment'] = dataset['TitleText'].apply(get_sentiment_score)\n",
    "\n",
    "new_dataframe = dataset[['Title', 'Review', 'Date', 'Rating', 'ReviewText', 'ReviewSentiment', 'TitleText', 'TitleSentiment']]\n",
    "\n",
    "print(new_dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfec7fb",
   "metadata": {},
   "source": [
    "## ETAPA 3: MACHINE LEARNING.\n",
    "\n",
    "### Paso 1. Asigne a la variable X las columnas numéricas menos \"Rating\"; asigne a la variable y la columna \"Rating\", seleccione únicamente las filas sin datos nulos (no elimine ni trate las filas con datos nulos, esas se usarán para predecir)\n",
    "\n",
    "### Paso 2. Divida en una muestra de entrenamiento y en una muestra de pruebas, estratifique en base a la proporción de la variable objetivo. El tamaño de la muestra para entrenamiento debe ser del 85%. Asigne una semilla para poder reproducir los resultados.\n",
    "\n",
    "### Paso 3. Entrene los siguientes modelos:\n",
    "#### - KNN para clasificación\n",
    "#### - SVM para clasificación\n",
    "#### - RandomForest para clasificación\n",
    "\n",
    "### Paso 4. Evalúe el rendimiento de los modelos (puede usar accuracy) creados en el paso previo, muestre las predicciones realizadas y compare con las etiquetas reales.\n",
    "\n",
    "### Paso 5. Debido a que este es un problema de clasificación, pero hay varias clases que son originalmente numéricas, se puede aplicar también una métrica de evaluación para regresión. Aplique el RMSE a las predicciones y las etiquetas reales, analice el resultado.\n",
    "\n",
    "### Paso 6. Utilice el modelo que se comportó mejor para predecir el \"Rating\" de las filas que tienen ese dato nulo, revise manualmente si la calificación predicha es consistente con el comentario en la reseña.\n",
    "\n",
    "### Paso 7. Escriba sus conclusiones al respecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4475d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (696, 2)\n",
      "Dimensiones de X_test: (174, 2)\n",
      "Dimensiones de y_train: (696,)\n",
      "Dimensiones de y_test: (174,)\n"
     ]
    }
   ],
   "source": [
    "#PASO 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.select_dtypes(include=['float64', 'int64']).drop(columns=['Rating'])\n",
    "\n",
    "y = dataset['Rating']\n",
    "\n",
    "combined_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "combined_data = combined_data.dropna()\n",
    "\n",
    "X = combined_data.drop(columns=['Rating'])\n",
    "y = combined_data['Rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de X_test:\", X_test.shape)\n",
    "print(\"Dimensiones de y_train:\", y_train.shape)\n",
    "print(\"Dimensiones de y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 1 redes neuronales\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "X = dataset.select_dtypes(include=[np.number]).drop(\"Rating\", axis=1)\n",
    "y = dataset[\"Rating\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_features = X.columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1) \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(preprocessor.fit_transform(X_train), y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "y_pred = model.predict(preprocessor.transform(X_test))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "faef168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (739, 2)\n",
      "Dimensiones de X_test: (131, 2)\n",
      "Dimensiones de y_train: (739,)\n",
      "Dimensiones de y_test: (131,)\n"
     ]
    }
   ],
   "source": [
    "#PASO 2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.select_dtypes(include=['float64', 'int64']).drop(columns=['Rating'])\n",
    "\n",
    "y = dataset['Rating']\n",
    "\n",
    "combined_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "combined_data = combined_data.dropna()\n",
    "\n",
    "X = combined_data.drop(columns=['Rating'])\n",
    "y = combined_data['Rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de X_test:\", X_test.shape)\n",
    "print(\"Dimensiones de y_train:\", y_train.shape)\n",
    "print(\"Dimensiones de y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 2 redes neuronales\n",
    "\n",
    "X = dataset.select_dtypes(include=[np.number]).drop(\"Rating\", axis=1)\n",
    "y = dataset[\"Rating\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "numeric_features = X.columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  \n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(preprocessor.fit_transform(X_train), y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "y_pred = model.predict(preprocessor.transform(X_test))\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84c6add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - KNN: 0.40458015267175573\n",
      "Accuracy - SVM: 0.48854961832061067\n",
      "Accuracy - RandomForest: 0.3816793893129771\n",
      "\n",
      "Classification Report - KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "         2.0       0.00      0.00      0.00         2\n",
      "         3.0       0.00      0.00      0.00         2\n",
      "         4.0       0.00      0.00      0.00         2\n",
      "         5.0       0.25      0.25      0.25         4\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         8\n",
      "         8.0       0.14      0.14      0.14        14\n",
      "         9.0       0.22      0.15      0.18        26\n",
      "        10.0       0.57      0.71      0.63        65\n",
      "\n",
      "    accuracy                           0.40       131\n",
      "   macro avg       0.12      0.13      0.12       131\n",
      "weighted avg       0.35      0.40      0.37       131\n",
      "\n",
      "\n",
      "Classification Report - SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "         2.0       0.00      0.00      0.00         2\n",
      "         3.0       0.00      0.00      0.00         2\n",
      "         4.0       0.00      0.00      0.00         2\n",
      "         5.0       0.50      0.25      0.33         4\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         8\n",
      "         8.0       0.00      0.00      0.00        14\n",
      "         9.0       0.00      0.00      0.00        26\n",
      "        10.0       0.50      0.97      0.66        65\n",
      "\n",
      "    accuracy                           0.49       131\n",
      "   macro avg       0.10      0.12      0.10       131\n",
      "weighted avg       0.26      0.49      0.34       131\n",
      "\n",
      "\n",
      "Classification Report - RandomForest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.33      0.25      0.29         4\n",
      "         2.0       0.00      0.00      0.00         2\n",
      "         3.0       0.00      0.00      0.00         2\n",
      "         4.0       0.00      0.00      0.00         2\n",
      "         5.0       0.67      0.50      0.57         4\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.33      0.25      0.29         8\n",
      "         8.0       0.00      0.00      0.00        14\n",
      "         9.0       0.19      0.23      0.21        26\n",
      "        10.0       0.56      0.60      0.58        65\n",
      "\n",
      "    accuracy                           0.38       131\n",
      "   macro avg       0.21      0.18      0.19       131\n",
      "weighted avg       0.37      0.38      0.37       131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#PASO 3\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X = dataset.select_dtypes(include=['float64', 'int64']).drop(columns=['Rating'])\n",
    "\n",
    "y = dataset['Rating']\n",
    "\n",
    "combined_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "combined_data = combined_data.dropna()\n",
    "\n",
    "X = combined_data.drop(columns=['Rating'])\n",
    "y = combined_data['Rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy - KNN:\", accuracy_score(y_test, knn_predictions))\n",
    "print(\"Accuracy - SVM:\", accuracy_score(y_test, svm_predictions))\n",
    "print(\"Accuracy - RandomForest:\", accuracy_score(y_test, rf_predictions))\n",
    "\n",
    "print(\"\\nClassification Report - KNN:\")\n",
    "print(classification_report(y_test, knn_predictions))\n",
    "\n",
    "print(\"\\nClassification Report - SVM:\")\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "print(\"\\nClassification Report - RandomForest:\")\n",
    "print(classification_report(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 3 redes neuronales\n",
    "\n",
    "X = dataset.select_dtypes(include=[np.number]).drop(\"Rating\", axis=1)\n",
    "y = dataset[\"Rating\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "numeric_features = X.columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    'KNN': KNeighborsRegressor(),\n",
    "    'SVM': SVR(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'NeuralNetwork': keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1) \n",
    "    ])\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'NeuralNetwork':\n",
    "      \n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.fit(preprocessor.fit_transform(X_train), y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "     \n",
    "        y_pred = model.predict(preprocessor.transform(X_test))\n",
    "    else:\n",
    "        \n",
    "        model.fit(preprocessor.fit_transform(X_train), y_train)\n",
    "  \n",
    "        y_pred = model.predict(preprocessor.transform(X_test))\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name} Mean Squared Error on Test Set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc509e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - KNN: 0.40458015267175573\n",
      "Accuracy - SVM: 0.48854961832061067\n",
      "Accuracy - RandomForest: 0.366412213740458\n",
      "\n",
      "Predicciones vs Etiquetas Reales:\n",
      "     Real   KNN   SVM  RandomForest\n",
      "444   7.0  10.0  10.0          10.0\n",
      "762   2.0   1.0  10.0           1.0\n",
      "725  10.0  10.0  10.0          10.0\n",
      "815  10.0  10.0  10.0          10.0\n",
      "738  10.0   8.0  10.0           8.0\n"
     ]
    }
   ],
   "source": [
    "#PASO 4\n",
    "print(\"Accuracy - KNN:\", accuracy_score(y_test, knn_predictions))\n",
    "print(\"Accuracy - SVM:\", accuracy_score(y_test, svm_predictions))\n",
    "print(\"Accuracy - RandomForest:\", accuracy_score(y_test, rf_predictions))\n",
    "\n",
    "predictions_df = pd.DataFrame({'Real': y_test, 'KNN': knn_predictions, 'SVM': svm_predictions, 'RandomForest': rf_predictions})\n",
    "print(\"\\nPredicciones vs Etiquetas Reales:\")\n",
    "print(predictions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 4 redes neuronales\n",
    "\n",
    "X = dataset.select_dtypes(include=[np.number]).drop(\"Rating\", axis=1)\n",
    "y = dataset[\"Rating\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "numeric_features = X.columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "models = {\n",
    "    'KNN': KNeighborsRegressor(),\n",
    "    'SVM': SVR(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'NeuralNetwork': keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1) \n",
    "    ])\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'NeuralNetwork':\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.fit(preprocessor.fit_transform(X_train), y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "  \n",
    "        y_pred = model.predict(preprocessor.transform(X_test))\n",
    "    else:\n",
    "     \n",
    "        model.fit(preprocessor.fit_transform(X_train), y_train)\n",
    "\n",
    "        y_pred = model.predict(preprocessor.transform(X_test))\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name} Mean Squared Error on Test Set: {mse}\")\n",
    "\n",
    "    sample_predictions = pd.DataFrame({'Prediction': y_pred.flatten(), 'Actual': y_test.values.flatten()})\n",
    "    print(f\"\\nSample Predictions for {name}:\\n\")\n",
    "    print(sample_predictions.head())\n",
    "    print(\"\\n----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4f5b74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE - KNN: 2.587403425936825\n",
      "RMSE - SVM: 2.7794227529641216\n",
      "RMSE - RandomForest: 2.57557523405152\n"
     ]
    }
   ],
   "source": [
    "#PASO 5\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "X = dataset.select_dtypes(include=['float64', 'int64']).drop(columns=['Rating'])\n",
    "\n",
    "y = dataset['Rating']\n",
    "\n",
    "combined_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "combined_data = combined_data.dropna()\n",
    "\n",
    "X = combined_data.drop(columns=['Rating'])\n",
    "y = combined_data['Rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "rmse_knn = sqrt(mean_squared_error(y_test, knn_predictions))\n",
    "rmse_svm = sqrt(mean_squared_error(y_test, svm_predictions))\n",
    "rmse_rf = sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "\n",
    "print(\"RMSE - KNN:\", rmse_knn)\n",
    "print(\"RMSE - SVM:\", rmse_svm)\n",
    "print(\"RMSE - RandomForest:\", rmse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 5 redes neuronales\n",
    "\n",
    "bins = [0, 1, 2, 3, 4, 5]\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "dataset['Clasificacion'] = pd.cut(dataset['Rating'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "X = dataset.select_dtypes(include=[np.number]).drop(\"Clasificacion\", axis=1)\n",
    "y = dataset[\"Clasificacion\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "numeric_features = X.columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'NeuralNetwork': keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(len(np.unique(y)), activation='softmax')  \n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'NeuralNetwork':\n",
    "\n",
    "        y_train_one_hot = tf.keras.utils.to_categorical(y_train - 1, num_classes=len(np.unique(y)))\n",
    "        y_test_one_hot = tf.keras.utils.to_categorical(y_test - 1, num_classes=len(np.unique(y)))\n",
    "\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(preprocessor.fit_transform(X_train), y_train_one_hot, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "     \n",
    "        y_pred = np.argmax(model.predict(preprocessor.transform(X_test)), axis=1) + 1\n",
    "    else:\n",
    "\n",
    "        model.fit(preprocessor.fit_transform(X_train), y_train)\n",
    "\n",
    "        y_pred = model.predict(preprocessor.transform(X_test))\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy on Test Set: {accuracy}\")\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name} Mean Squared Error on Test Set: {mse}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7151df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ReviewSentiment  TitleSentiment  Rating\n",
      "0             0.8996          0.5106     8.0\n",
      "1             0.9887          0.8074     9.0\n",
      "2             0.9825          0.1531     9.0\n",
      "3             0.8300          0.0000    10.0\n",
      "4             0.9623          0.0772    10.0\n",
      "..               ...             ...     ...\n",
      "873           0.8750          0.4404    10.0\n",
      "874           0.9933          0.9349     9.0\n",
      "875           0.9732          0.4404    10.0\n",
      "876           0.9963          0.5719    10.0\n",
      "877           0.9860          0.6369     9.0\n",
      "\n",
      "[878 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#PASO 6\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = dataset.select_dtypes(include=['float64', 'int64']).drop(columns=['Rating'])\n",
    "\n",
    "y = dataset['Rating']\n",
    "\n",
    "combined_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "rows_with_null_rating = combined_data[combined_data['Rating'].isnull()]\n",
    "rows_without_null_rating = combined_data[combined_data['Rating'].notnull()]\n",
    "\n",
    "X = rows_without_null_rating.drop(columns=['Rating'])\n",
    "y = rows_without_null_rating['Rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predicted_ratings = rf_model.predict(rows_with_null_rating.drop(columns=['Rating']))\n",
    "\n",
    "combined_data.loc[combined_data['Rating'].isnull(), 'Rating'] = predicted_ratings\n",
    "\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0221ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 6 redes neuronales\n",
    "\n",
    "rows_to_predict = dataset[dataset['Rating'].isnull()]\n",
    "\n",
    "X_train = dataset.select_dtypes(include=[np.number]).drop(\"Rating\", axis=1)\n",
    "y_train = dataset[\"Rating\"]\n",
    "\n",
    "numeric_features = X_train.columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "best_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_preprocessed.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1) \n",
    "])\n",
    "\n",
    "best_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "best_model.fit(X_train_preprocessed, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "X_to_predict = preprocessor.transform(rows_to_predict.select_dtypes(include=[np.number]))\n",
    "\n",
    "predicted_ratings = best_model.predict(X_to_predict)\n",
    "\n",
    "rows_to_predict['Rating'] = predicted_ratings\n",
    "\n",
    "print(rows_to_predict[['Title', 'Review', 'Date', 'Rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd01758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASO 7\n",
    "#la revisión manual del rating predicho es escencial para validar la coherencia de las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c77cc",
   "metadata": {},
   "source": [
    "## ETAPA 4: PRESENTACIÓN DEL PROYECTO.\n",
    "### Al igual que en los proyectos anteriores, suba su trabajo terminado a un repositorio nuevo de su GitHub.\n",
    "### Escriba un archivo \"README\" y describa el proceso seguido en este trabajo.\n",
    "### Al entregar este trabajo, debe subir este archivo, el archivo de instrucciones y el enlace al repositorio público de GitHub con este trabajo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
